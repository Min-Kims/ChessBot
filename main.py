import os
import streamlit as st
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain_community.vectorstores.faiss import FAISS
from langserve import RemoteRunnable
from langchain_openai import ChatOpenAI
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler


# â­ï¸ Embedding ì„¤ì •
# USE_BGE_EMBEDDING = True ë¡œ ì„¤ì •ì‹œ HuggingFace BAAI/bge-m3 ì„ë² ë”© ì‚¬ìš© (2.7GB ë‹¤ìš´ë¡œë“œ ì‹œê°„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)
# USE_BGE_EMBEDDING = False ë¡œ ì„¤ì •ì‹œ OpenAIEmbeddings ì‚¬ìš© (OPENAI_API_KEY ì…ë ¥ í•„ìš”. ê³¼ê¸ˆ)
USE_BGE_EMBEDDING = True

if not USE_BGE_EMBEDDING:
    # OPENAI API KEY ì…ë ¥
    # Embedding ì„ ë¬´ë£Œ í•œê¸€ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•˜ë©´ í•„ìš” ì—†ìŒ!
    os.environ["OPENAI_API_KEY"] = "OPENAI API KEY ì…ë ¥"

# â­ï¸ LangServe ëª¨ë¸ ì„¤ì •(EndPoint)
# 1) REMOTE ì ‘ì†: ë³¸ì¸ì˜ REMOTE LANGSERVE ì£¼ì†Œ ì…ë ¥
# (ì˜ˆì‹œ)
#LANGSERVE_ENDPOINT = "https://poodle-deep-marmot.ngrok-free.app/llm/"
#ë¯¼ ì˜ˆì‹œ LANGSERVE_ENDPOINT = "https://8081-182-212-208-243.ngrok-free.app/llm/"

# 2) LocalHost ì ‘ì†: ëì— ë¶™ëŠ” N4XyA ëŠ” ê°ì ë‹¤ë¥´ë‹ˆ
# http://localhost:8000/llm/playground ì—ì„œ python SDK ì—ì„œ í™•ì¸!
LANGSERVE_ENDPOINT = "http://localhost:8000/llm/c/N4XyA"
#ê¹€ë¯¼ http://localhost:8000/xionic/c/N4XyA

# í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„± @Mineru
if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

# í”„ë¡¬í”„íŠ¸ë¥¼ ììœ ë¡­ê²Œ ìˆ˜ì •í•´ ë³´ì„¸ìš”!
RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì²´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì²´ìŠ¤ ì„ ìƒë‹˜ì²˜ëŸ¼ ë‹µë³€í•˜ì„¸ìš”. ëª¨ë“  ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì…ë‹ˆë‹¤. ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.  
Question: {question} 
Context: {context} 
Answer:"""

st.set_page_config(
    page_title="â™Ÿï¸Chess Chatbotâ™Ÿï¸",
    page_icon="â™Ÿï¸"
)

st.markdown("""
<style>
            
img {
    max-heigh: 300px;
}
.streamlit-expanderContent div {
    display : flex;
    justify-content: center;
    font-size: 20px;
            
.st-emotion-cache-j6qv4b e1nzilvr5 div {
    font-size: 10px;
}
}
[data-testid="stExpanderToggleIcon"] {
    visibility: hidden;
}      
</style>    
""", unsafe_allow_html=True)


st.title("Chess GPTâ™Ÿï¸")
st.markdown("Made by. KM")

#ì‚¬ì§„ ì¶”ê°€
picture_path = "C:/Users/aasxs/OneDrive/ë°”íƒ• í™”ë©´/ì‚¬ì§„/DALLÂ·E 2024-05-23 23.27.44 - A high-tech chessboard with chrome and glass pieces poised for a game, viewed from a side angle. The background is a high-tech laboratory with robotic.webp"

# expander : ì ‘ê¸°í¼ì¹˜ê¸°, label = ì´ë¦„, expanded=True : ê¸°ë³¸ì ìœ¼ë¡œ í¼ì¹˜ê¸°
# with í•˜ìœ„ì— ë„£ì€ ê²ƒë“¤ì´ ì „ë¶€ í¼ì¹˜ê¸°, ì ‘ê¸°ì— í¬í•¨
with st.expander(label = "Chess Image" ,expanded=True):
    st.image(picture_path)

#ì²´ìŠ¤ ì„ ìˆ˜ë“¤ ì»¬ëŸ¼ë³„ë¡œ ì‚¬ì§„ ì¶”ê°€í•˜ê¸°
initial_grandmaster = [
    {
        "name" : "Magnus Carlsen",
        "country" : "Norway",
        "rating" : 2830,
        "image_url" : "https://images.chesscomfiles.com/uploads/v1/master_player/0541e1fa-ad51-11eb-8ff2-ef206b1039dc.28e1af01.250x250o.854dcaef7310@2x.jpg"
    },
    {
        "name" : "Hikaru",
        "country" : "USA",
        "rating" : 2794,
        "image_url" : "https://images.chesscomfiles.com/uploads/v1/master_player/c9d9e712-b795-11eb-ad09-a98e159bb142.71b234af.250x250o.8ec3b4a5d8bb@2x.jpeg"
    },
    {
        "name" : "Fabiano Caruana",
        "country" : "USA",
        "rating" : 2805,
        "image_url" : "https://images.chesscomfiles.com/uploads/v1/master_player/c69f503e-9b37-11eb-9e62-fb361d491281.d47636d1.250x250o.d8a2bc99d402@2x.jpeg"
    },
    {
        "name" : "Ian Nepomniachtchi",
        "country" : "Rusia",
        "rating" : 2770,
        "image_url" : "https://images.chesscomfiles.com/uploads/v1/master_player/9717c30a-ae0b-11eb-8a19-935f50840bf3.88bdb385.250x250o.0ecb6f67541f@2x.png"
    },
    {
        "name" : "Nodirbek Abdusattorov",
        "country" : "Uzbekistan",
        "rating" : 2766,
        "image_url" : "https://images.chesscomfiles.com/uploads/v1/master_player/bdf65d10-6151-11ed-b155-8b0a0660e799.ae11d334.250x250o.f4e88889cf1e@2x.jpg"
    },
    {
        "name" : "Gukesh Dommaraju",
        "country" : "India",
        "rating" : 2763,
        "image_url" : "https://images.chesscomfiles.com/uploads/v1/master_player/918644b8-e1b9-11ee-9575-f74f04d3e9fc.a89af8fc.250x250o.6164fe81dcb6@2x.png"
    },
    {
        "name" : "Hanrim Kang",
        "country" : "South Korea",
        "rating" : 1400,
        "image_url" : "https://scontent-ssn1-1.xx.fbcdn.net/v/t1.18169-9/15781217_610282815822658_68050988425839739_n.jpg?_nc_cat=110&ccb=1-7&_nc_sid=5f2048&_nc_ohc=CZRLXZe7uEYQ7kNvgGdY7cV&_nc_ht=scontent-ssn1-1.xx&oh=00_AYA1O1cb0cXC7_Dpg0oN7RaZbaBtaNLff5V2clnVbepV5g&oe=6676BEEE"
    },
    {
        "name" : "Min Kim",
        "country" : "South Korea",
        "rating" : 1120,
        "image_url" : "C:/Users/aasxs/OneDrive/ë°”íƒ• í™”ë©´/ì‚¬ì§„/kimmin_profiles.jpg"
    }
    
]

if "grandmaster" not in st.session_state:
    st.session_state.grandmaster = initial_grandmaster

#ë‚˜ë¼ ë“±ë¡
country_dict = {
    "USA" : "ğŸ’µ",
    "Korea" : ":kr:",
    "Norway" : "norway",
    "India" : "india",
    "Uzbekistan" : "ğŸ˜Š",
    "Rusia" : "ğŸ˜‚"
}

#ì„ ìˆ˜ë“±ë¡ 
# with st.form(key="form"):
#     col1, col2, col3 = st.columns(3)
#     with col1:
#         name = st.text_input(label="Player Name")
#     with col2:
#         countries = st.multiselect(
#             label="Country", 
#             options=list(country_dict.keys()),
#             max_selections= 2
#         )
#     with col3:
#         ratings = st.text_input(label="Rating")
#     image_url = st.text_input(label="Player Image")
#     submit = st.form_submit_button(label= "Submit")
#     if submit:
#         if not name:
#             st.error("Playerì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
#         elif len(countries) == 0:
#             st.error("ë‚˜ë¼ë¥¼ ì ì–´ë„ 1ê°œ ì´ìƒ ì…ë ¥í•´ì£¼ì„¸ìš”.")
#         elif not ratings:
#             st.error("ratingì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
#         else:
#             st.success("ì„ ìˆ˜ë“±ë¡ì„ ì„±ê³µí•˜ì˜€ìŠµë‹ˆë‹¤.")
#             grandmaster.append({
#                 "name" : name,
#                 "country" : countries,
#                 "rating" : ratings,
#                 "image_url" : image_url if image_url else "./images/default.png"
#             })


#ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    file = st.file_uploader(
        "ê¸°ë³´ ì—…ë¡œë“œ",
        type=["pdf", "txt", "docx"],
    )
    #Testí•´ë³¸ ê²ƒ 
    with st.form(key="form"):
        col1, col2, col3 = st.columns(3)
        with col1:
            name = st.text_input(label="Player Name")
        with col2:
            countries = st.multiselect(
                label="Country", 
                options=list(country_dict.keys()),
                max_selections= 2
            )
        with col3:
            ratings = st.text_input(label="Rating")
        image_url = st.text_input(label="Player Image")
        submit = st.form_submit_button(label= "Submit")
        if submit:
            if not name:
                st.error("Playerì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            elif len(countries) == 0:
                st.error("ë‚˜ë¼ë¥¼ ì ì–´ë„ 1ê°œ ì´ìƒ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            elif not ratings:
                st.error("ratingì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                st.success("ì„ ìˆ˜ë“±ë¡ì„ ì„±ê³µí•˜ì˜€ìŠµë‹ˆë‹¤.")
                initial_grandmaster.append({
                    "name" : name,
                    "country" : countries,
                    "rating" : ratings,
                    "image_url" : image_url if image_url else "./images/default.png"
                })


#ì´ë¯¸ì§€ ë°°ì¹˜ (ì„ ìˆ˜ë“¤) 
for i in range(0, len(st.session_state.grandmaster), 3):
    row_grandmaster = st.session_state.grandmaster[i:i+3]
    cols = st.columns(3)
    for j in range(len(row_grandmaster)):
        with cols[j]:
            player = row_grandmaster[j]
            with st.expander(label=f"**{i+j+1}.{player["name"]}**", expanded=True):
                st.image(player["image_url"])
                #ì‚­ì œë²„íŠ¼ 
                delete_button = st.button(label="Delete", key=i+j, use_container_width=True)
                if delete_button:
                    del st.session_state.grandmaster[i+j]
                    st.rerun()


#####Logic ê´€ë ¨#####


if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]


def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)


def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))


def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)


@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "(?<=\. )", " ", ""],
        length_function=len,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=text_splitter)

    if USE_BGE_EMBEDDING:
        # BGE Embedding: @Mineru
        model_name = "BAAI/bge-m3"
        # GPU Device ì„¤ì •:
        # - NVidia GPU: "cuda"
        # - Mac M1, M2, M3: "mps"
        # - CPU: "cpu"
        model_kwargs = {
            # "device": "cuda"
            # "device": "mps"   #Default
            "device": "cpu"    #ê¹€ë¯¼ ìˆ˜ì •í–ˆìŒ
        }
        encode_kwargs = {"normalize_embeddings": True}
        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs,
        )
    else:
        embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, embedding=cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever


def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)








if file:
    retriever = embed_file(file)

print_history()


if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        # ngrok remote ì£¼ì†Œ ì„¤ì •
        ollama = RemoteRunnable(LANGSERVE_ENDPOINT)
        # LM Studio ëª¨ë¸ ì„¤ì •
        # ollama = ChatOpenAI(
        #     base_url="http://localhost:1234/v1",
        #     api_key="lm-studio",
        #     model="teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf",
        #     streaming=True,
        #     callbacks=[StreamingStdOutCallbackHandler()],  # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì¶”ê°€
        # )
        chat_container = st.empty()
        if file is not None:
            prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            rag_chain = (
                {
                    "context": retriever | format_docs,
                    "question": RunnablePassthrough(),
                }
                | prompt
                | ollama
                | StrOutputParser()
            )
            # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜ë¥¼ ì…ë ¥í•˜ê³ , ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
            answer = rag_chain.stream(user_input)  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))
        else:
            prompt = ChatPromptTemplate.from_template(
                "ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”:\n{input}"
            )

            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            chain = prompt | ollama | StrOutputParser()

            answer = chain.stream(user_input)  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))
